{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c8a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Important Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Web Scraping\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "import time\n",
    "from time import sleep\n",
    "from datetime import date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Sentiment Analysis\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import en_core_web_sm \n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe('spacytextblob')\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To get the time in which output is generated\n",
    "start_time = time.time()\n",
    "dte=date.today()\n",
    "dte1=dte.strftime(\"%d%m%Y\")\n",
    "date=dte.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# output file path\n",
    "path=r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsandSentimentsAnalysis_\"+dte1+\".xlsx\"\n",
    "wb=pd.ExcelWriter(path)\n",
    "test_df= pd.DataFrame(columns=['Date', 'URL', 'Category', 'Headline', 'News'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4536a",
   "metadata": {},
   "source": [
    "# Web Scraping from 7 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655271c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economic Times\n",
    "url=\"https://economictimes.indiatimes.com\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all(\"a\")\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if \"/articleshow/\" in href:\n",
    "            if \"/hindi/\" not in href:\n",
    "                if \"/news/economy/\" in href:\n",
    "                    news=href\n",
    "                elif \"/markets/\" in href:\n",
    "                    if \"/expert-view/\" not in href:\n",
    "                        news=href\n",
    "                elif \"/Small-biz/\" in href:\n",
    "                    news=href\n",
    "                elif \"/industry/\" in href:\n",
    "                    if \"entertainment\" not in href:\n",
    "                        if \"electronics\" not in href:\n",
    "                            if \"/advertising/\" not in href:\n",
    "                                news=href\n",
    "                elif \"/tech/ites/\" in href:\n",
    "                    news=href\n",
    "                elif \"/small-biz/\" in href:\n",
    "                    news=href\n",
    "                else:\n",
    "                    continue\n",
    "                if \"https\" not in news:\n",
    "                    news=url+news\n",
    "                cat1=news.split(\".com\")[1]\n",
    "                cat2=cat1.split(\"/\")\n",
    "                cat_fin=cat2[1]+\"/\"+cat2[2]#+\"/\"+cat2[3]print(\"Category: \"+cat_fin)\n",
    "                #print(news)\n",
    "                #arti=Article(news)\n",
    "                #arti.download()\n",
    "                #arti.parse()\n",
    "                #arti.nlp()\n",
    "                #txt=arti.summary\n",
    "                #head=arti.title\n",
    "                try:\n",
    "                    pg2=requests.get(news, verify=False)\n",
    "                    soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "                    prtify2=soup2.prettify()\n",
    "                    head=prtify2.split('\"headline\": \"')[1]\n",
    "                    headline=head.split('\"description\":')[0]\n",
    "                    headline=headline.rstrip().strip('\",')\n",
    "                    arti=prtify2.split('\"articleBody\":\"')[1]\n",
    "                    txt=arti.split('\"image\": {')[0]\n",
    "                    txt=txt.rstrip().strip('\",')\n",
    "                except:\n",
    "                    continue\n",
    "                table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat_fin, \"Headline\":headline,  \"News\":txt}, index=[0])\n",
    "                test_df=test_df.append(table)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ba809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Standard\n",
    "url=\"https://www.business-standard.com/\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all('a')\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        elif \".html\" in href:\n",
    "            if \"/finance/\" in href:\n",
    "                news=href\n",
    "                cat=\"finance\"\n",
    "            elif \"/international/\" in href:\n",
    "                news=href\n",
    "                cat=\"international\"\n",
    "            elif \"/technology/\" in href:\n",
    "                news=href\n",
    "                cat=\"technology\"\n",
    "            elif \"/economy-policy/\" in href:\n",
    "                news=href\n",
    "                cat=\"economy-policy\"\n",
    "            elif \"/current-affairs/\" in href:\n",
    "                news=href\n",
    "                cat=\"/current-affairs\"\n",
    "            elif \"/companies/\" in href:\n",
    "                news=href\n",
    "                cat=\"/companies\"\n",
    "            elif \"/automobile/\" in href:\n",
    "                news= href\n",
    "                cat=\"/automobile\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if \"http\" not in news:\n",
    "            news=url+news\n",
    "        try:\n",
    "            pg2=requests.get(news, verify=False)\n",
    "            soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "            prtify2=soup2.prettify()\n",
    "            head=prtify2.split('\"headline\":')[1]\n",
    "            headline=head.split('\"author\"')[0]\n",
    "            headline=headline.rstrip().strip('\",')\n",
    "            arti=prtify2.split('\"articleBody\":')[1]\n",
    "            txt=arti.split('\"articleSection\":')[0]\n",
    "            txt=txt.rstrip().strip('\",').strip()\n",
    "            table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat, \"Headline\":headline,\"News\":txt}, index=[0])\n",
    "            test_df=test_df.append(table)\n",
    "        except:\n",
    "            continue\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money Control\n",
    "url=\"https://www.moneycontrol.com/\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all('a')\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if \".html\" in href:\n",
    "            if \"abrasives\" in href:\n",
    "                continue\n",
    "            elif \"/india/\" in href:\n",
    "                news=href\n",
    "                cat=\"/india\"\n",
    "            elif \"/business/\" in href:\n",
    "                news=href\n",
    "                cat=\"/business\"\n",
    "            elif \"/technology/\" in href:\n",
    "                news=href\n",
    "                cat=\"/technology\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if \"http\" not in news:\n",
    "            news=url+news\n",
    "        try:\n",
    "            pg2=requests.get(news, verify=False)\n",
    "            soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "            prtify2=soup2.prettify()\n",
    "            if \"To view the full content of this article\" in prtify2:\n",
    "                continue\n",
    "            head=prtify2.split('\"headline\": \"')[1]\n",
    "            headline=head.split('\"description\"')[0]\n",
    "            headline=headline.rstrip().strip('\",')\n",
    "            arti=prtify2.split('\"articleBody\":\"')[1]\n",
    "            txt=arti.split('\"articleSection\":')[0]\n",
    "            txt=txt.rstrip().strip('\",')\n",
    "            table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat, \"Headline\":headline, \"News\":txt}, index=[0])\n",
    "            test_df=test_df.append(table)\n",
    "        except:\n",
    "            continue\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Mint\n",
    "url=\"https://www.livemint.com/\"\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all('a')\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if \".html\" in href:\n",
    "            #if \"api.whatsapp.com/\" in href:\n",
    "                #continue\n",
    "            if \"/companies/\" in href:\n",
    "                news=href\n",
    "                cat=\"/companies\"\n",
    "            elif \"/market/\" in href:\n",
    "                news=href\n",
    "                cat=\"/market\"\n",
    "            elif \"/news/india/\" in href:\n",
    "                news=href\n",
    "                cat=\"/india\"\n",
    "            elif \"/technology/\" in href:\n",
    "                news=href\n",
    "                cat=\"/technology\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if \"http\" not in news:\n",
    "            news=url+news\n",
    "        try:\n",
    "            pg2=requests.get(news, verify=False)\n",
    "            soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "            prtify2=soup2.prettify()\n",
    "            head=prtify2.split('\"headline\": \"')[1]\n",
    "            headline=head.split('description\"')[0]\n",
    "            headline=headline.rstrip().strip('\",')\n",
    "            arti=prtify2.split('\"articleBody\": \"')[1]\n",
    "            txt=arti.split('\"alternativeHeadline\":')[0]\n",
    "            txt=txt.rstrip().strip('\",')\n",
    "            table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat, \"Headline\":headline, \"News\":txt}, index=[0])\n",
    "            test_df=test_df.append(table)\n",
    "        except:\n",
    "            continue\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23952f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindustan Times\n",
    "url=\"https://www.hindustantimes.com/business\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all('a')\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        elif \".html\" in href:\n",
    "            if \"/videos/\" in href:\n",
    "                continue\n",
    "            elif \"bangla\" in href:\n",
    "                continue\n",
    "            elif \"marathi\" in href:\n",
    "                continue\n",
    "            elif \"/nation-and-world/\" in href:\n",
    "                news=href\n",
    "                cat=\"/nation-and-world\"\n",
    "            elif \"/tech/\" in href:\n",
    "                news=href\n",
    "                cat=\"/tech\"\n",
    "            elif \"/india-news/\" in href:\n",
    "                news=href\n",
    "                cat=\"/india-news\"\n",
    "            elif \"/business-news/\" in href:\n",
    "                news= href\n",
    "                cat=\"/business-news\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if \"http\" not in news:\n",
    "            news=url+news\n",
    "            try:\n",
    "                pg2=requests.get(news, verify=False)\n",
    "                soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "                prtify2=soup2.prettify()\n",
    "                head=prtify2.split('\"headline\":\"')[1]\n",
    "                headline=head.split('\"description\":\"')[0]\n",
    "                headline=headline.rstrip().strip('\",')\n",
    "                arti=prtify2.split('\"articleBody\":\"')[1]\n",
    "                txt=arti.split('\"author\":')[0]\n",
    "                txt=txt.rstrip().strip('\",')\n",
    "                table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat, \"Headline\":headline, \"News\":txt}, index=[0])\n",
    "                test_df=test_df.append(table)\n",
    "            except:\n",
    "                continue\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News India Express\n",
    "url=\"https://www.newindianexpress.com/\"\n",
    "\n",
    "char=\"/\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all('a')\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if \"/nation/\" in href:#nation-and-world  ##/tech/  ##/india-news/\n",
    "            if \"/galleries/\" not in href:\n",
    "                news=href\n",
    "                cat=\"/nation\"\n",
    "        elif \"/business/\" in href:#/business-news##contains .html-->for all news\n",
    "            news=href\n",
    "            cat=\"/business\"\n",
    "        elif \"/states/\" in href:\n",
    "            #print(len(re.findall(char, href)))\n",
    "            if len(re.findall(char, href))>5:\n",
    "                #print(len(re.findall(char, href)))\n",
    "                news=href\n",
    "                cat=\"/states\"\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        if \"https\" not in news:\n",
    "            news=url+news\n",
    "        try:\n",
    "            pg2=requests.get(news, verify=False)\n",
    "            soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "            prtify2=soup2.prettify()\n",
    "            head=prtify2.split('headline\":\"')[1]\n",
    "            head=prtify2.split('\"headline\": \"')[1]#for asianage\n",
    "            headline=head.split('\",\"description\"')[0]\n",
    "            headline=head.split('\",\\n\"image\": {')[0]#for asianage\n",
    "            arti=prtify2.split('\"articleBody\":\"')[1]\n",
    "            txt=arti.split('\",\"wordCount\"')[0]\n",
    "            table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat, \"Headline\":headline, \"News\":txt}, index=[0])\n",
    "            test_df=test_df.append(table)\n",
    "        except:\n",
    "            continue\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a428df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times of India\n",
    "url=\"https://timesofindia.indiatimes.com/\"\n",
    "\n",
    "try:\n",
    "    pg=requests.get(url, verify=False)\n",
    "    soup=BeautifulSoup(pg.content, 'lxml')\n",
    "    prtify=soup.prettify()\n",
    "    link=soup.find_all(\"a\")\n",
    "    for lnk in link:\n",
    "        href=lnk.get('href')\n",
    "        if href is None:\n",
    "            continue\n",
    "        if \"/articleshow/\" in href:\n",
    "            if \"/hindi/\" not in href:\n",
    "                if \"/tech/ites/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/tech/ites\"\n",
    "                elif \"/india/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/india\"\n",
    "                elif \"/business/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/business\"\n",
    "                elif \"/tech/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/tech\"\n",
    "                elif \"/tech-news/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/tech-news\"\n",
    "                elif \"/world/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/world\"\n",
    "                elif \"/other-news/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/other-news\"\n",
    "                elif \"/gadgets-news/\" in href:#science, environment, health-and-fitness, gaming, travel, \n",
    "                    news=href\n",
    "                    cat_fin=\"/gadgets-news\"\n",
    "                elif \"/auto/news/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/auto/news\"\n",
    "                elif \"/electronics/\" in href:\n",
    "                    news=href\n",
    "                    cat_fin=\"/electronics\"\n",
    "                else:\n",
    "                    continue\n",
    "                if \"https\" not in news:\n",
    "                    news=url+news\n",
    "                try:\n",
    "                    if \"businessinsider\" in news:\n",
    "                        pg2=requests.get(news, verify=False)\n",
    "                        soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "                        prtify2=soup2.prettify()\n",
    "                        head=prtify2.split('\"headline\":')[1]\n",
    "                        headline=head.split('\"keywords\":')[0]\n",
    "                        headline=headline.rstrip().strip('\",').strip('\"')\n",
    "                        arti=prtify2.split('\"articleBody\":\"')[1]\n",
    "                        txt=arti.split('\"image\":')[0]\n",
    "                        txt=txt.strip('\",').strip('/').strip()\n",
    "                    elif \"timesofindia\" in news:\n",
    "                        pg2=requests.get(news, verify=False)\n",
    "                        soup2=BeautifulSoup(pg2.content, 'lxml')\n",
    "                        prtify2=soup2.prettify()\n",
    "                        head=prtify2.split('\"headline\":')[1]\n",
    "                        headline=head.split(',\"keywords\"')[0].strip('\"').strip()\n",
    "                        arti=prtify2.split('clearfix\">')[1]\n",
    "                        txt=arti.split('<h6>')[0].replace('\">','').replace('<br/>','').replace('</li>','').replace(\"<!-- -->\",'').replace('<li>','').replace(' </ul>','').replace('</ul>','').replace('</span>','').replace('<span class=\"em \">','').replace('</div>','').replace('</a>','').replace('frmappuse=\"1\">','').replace('styleObj=\"[object Object]\"','').replace(' class=\"\"','').replace(',&quot;','\"').replace(\"&#x27;\",\"'\").strip()\n",
    "                        txt=txt.replace('\",','').replace('/','').replace(\"</span>\",'').replace('<span class=\"span\">','').replace(\"<div class=\",'').replace(\"<\",'').replace(\">\",'').replace(\"=\",'').strip()\n",
    "                except:\n",
    "                    continue\n",
    "                table=pd.DataFrame({\"Date\":date, \"URL\":news, \"Category\":cat_fin, \"Headline\":headline, \"News\":txt}, index=[0])\n",
    "                test_df=test_df.append(table)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling news from all the sources\n",
    "test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d56b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Headline column\n",
    "test_df['Headline'] = test_df['Headline'].apply(lambda x: x.split('  \"image\"')[0])\n",
    "test_df['Headline'] = test_df['Headline'].apply(lambda x: x.split('  \"mainEntityOfPage\"')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning News column\n",
    "test_df['News'] = test_df['News'].apply(lambda x: x.replace(\"._x000D_\", \" \" ))\n",
    "test_df['News'] = test_df['News'].apply(lambda x: x.replace(\"_x000D_\", \" \" ))\n",
    "test_df['News'] = test_df['News'].apply(lambda x: x.replace(\"&amp;quot;\", \" \" ))\n",
    "\n",
    "#Replacing TOI junk news with headlines\n",
    "test_df[\"News\"]= test_df.apply(lambda x: x['News'].replace('sEzxj hide', str(x['Headline'])), axis=1)\n",
    "test_df['News'] = test_df['News'].apply(lambda x: x.replace(\"_2sdAr contentwrapper\", \" \" ))\n",
    "\n",
    "\n",
    "#Removing junnk news from Money Control\n",
    "test_df = test_df[~test_df['News'].str.contains('\"name\" : \"Moneycontrol News\"')]\n",
    "test_df = test_df[~test_df['News'].str.contains('prime-logo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e204781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning extracted data\n",
    "#Removing NAs\n",
    "new_df=  test_df.dropna(subset = 'News')\n",
    "# Removing Duplicates\n",
    "new_df = new_df.drop_duplicates('News')\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd948",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "punctuations = string.punctuation\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "test_text = new_df['News']\n",
    "# Lowercasing Text\n",
    "new_df['Clean_text'] = new_df['News'].apply(lambda x: ' '.join(x.lower() for x in str(x).split()))\n",
    "#new_df['Clean_text']= new_df['Clean_text'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in spacy_tokenizer(x)]))\n",
    "#new_df['Tokens'] = new_df['Clean_text'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in spacy_tokenizer(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "new_df['Clean_text']= new_df['Clean_text'].apply(lambda x: \" \".join([x for x in x.split() if x not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "new_df['Clean_text']= new_df['Clean_text'].apply(lambda x: \" \".join([x for x in x.split() if x not in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating SPACY Polarity Score\n",
    "polarity_score = []\n",
    "test_text = new_df['Clean_text']\n",
    "for text in test_text:\n",
    "    doc = nlp(text)\n",
    "    polarity = (doc._.polarity)\n",
    "    polarity_score.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b635acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing into Positive, Negative and Neutral on the basis of Polarity score\n",
    "new_df['Compound'] = polarity_score\n",
    "conditions = [\n",
    "    (new_df['Compound'] > 0.05),# Positive\n",
    "    (new_df['Compound'] < -0.05), # Negative\n",
    "    (new_df['Compound'] >= -0.05) & (new_df['Compound'] <= 0.05)] # Neutral\n",
    "\n",
    "values = [1,-1 ,0]# [1:Positive, -1: Negative, 0: Neutral]\n",
    "\n",
    "# New column on the basis of polarity\n",
    "new_df['Sentiment'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38275bfb",
   "metadata": {},
   "source": [
    "# Test Flags and Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fba0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# News Fraudulent Word Tagging\n",
    "words=[\"fraud\", \"conspiracy\", \"cheating\", \"forgery\", \"false evidence\", \"fake evidence\", \"breach of trust\", \"money laundering\", \n",
    "       \"corrupt\", \"abetment\",\"scam\", \"helped in crime\", \"helped in criminal\", \"extortion\", \"warrants pending\", \"warrant pending\", \n",
    "       \"sexual harassment\", \"fraudulent\", \"corruption\", \"delay in payment\", \"delayed payment\", \"cheque bounce\", \"bounced cheque\", \n",
    "       \"loan due\", \"taxes due\", \"liquidation\", \"winding up\", \"bankruptcy\", \"bankrupt\", \"debarred\", \"balcklisted\",\"labor issues\", \n",
    "       \"labour issues\", \"child labour\", \"child labor\", \"forced labor\", \"forced labour\", \"GMP audit observation\", \n",
    "       \"drt\", \"debt recovery tribunal\", \"debt recovery\",\"gst evasion\",\"gst\"]\n",
    "\n",
    "# Define the function which takes a string and a list of words to extract as inputs\n",
    "def listing_splitter(text, word):\n",
    "    # Try except to handle np.nans in input\n",
    "    try:\n",
    "        # Extract the list of flags\n",
    "        flags = [l for l in words if l in text.lower()]\n",
    "        # If any flags were extracted then return the list\n",
    "        if flags:\n",
    "            return flags\n",
    "        # Otherwise return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    except AttributeError:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the column\n",
    "new_df['Test_Flag'] = new_df['Clean_text'].apply(lambda x: listing_splitter(x, words))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7280ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new function with Named Entity Recognition and applying it to column\n",
    "def spacy_entity(new_df):    \n",
    "    df1 = nlp(new_df)\n",
    "    df2 = [[w.text,w.label_] for w in df1.ents]\n",
    "    return df2\n",
    "new_df['NER'] = new_df['News'].apply(spacy_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Organizational entities from Text\n",
    "def organisation(v):\n",
    "    r = [(x,y) for x,y in v if ('ORG' == y)]\n",
    "    return [(x) for x,y in r]\n",
    "\n",
    "new_df['Entities'] = new_df['NER'].apply(lambda v: organisation(v))\n",
    "new_df['Entities']= new_df['Entities'].apply(lambda x: list(set(x)) if x is not np.nan else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def person(v):\n",
    "    r = [(x,y) for x,y in v if ('PERSON' == y)]\n",
    "    return [(x) for x,y in r]\n",
    "\n",
    "new_df['Person'] = new_df['NER'].apply(lambda v: person(v))\n",
    "new_df['Person']= new_df['Person'].apply(lambda x: list(set(x)) if x is not np.nan else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4150e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_flag_df = new_df.loc[new_df['Test_Flag'].isnull() == False] \n",
    "test_flag_df = test_flag_df[['Date', 'URL', 'Category', 'News','Headline', 'Test_Flag', 'Entities']]\n",
    "test_flag_df.to_excel(r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsKeywordsEntities_05122022.xlsx\")\n",
    "test_flag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39924b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique cases of Fraud Flags found in the news\n",
    "new_df['Test_Flag'].explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d5d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to our path\n",
    "new_df.to_excel(wb, sheet_name=\"News\", index=False)\n",
    "wb.save()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbb84c",
   "metadata": {},
   "source": [
    "# Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c253ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Training Dataframe by putting T-3/ T-4 Files\n",
    "df1 = pd.read_excel(r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsandSentimentsAnalysis_04112022.xlsx\")\n",
    "df2 = pd.read_excel(r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsandSentimentsAnalysis_07112022.xlsx\")\n",
    "df3 = pd.read_excel(r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsandSentimentsAnalysis_08112022.xlsx\")\n",
    "df4 = pd.read_excel(r\"C:\\Users\\sawanta\\Downloads\\News_Sentiment_Spacy\\News\\NewsandSentimentsAnalysis_11112022.xlsx\")\n",
    "\n",
    "# Compiling all 3/4 files\n",
    "train_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Dropping NA values\n",
    "train_df = train_df.dropna(subset = 'News')\n",
    "\n",
    "# Dropping Duplicate values\n",
    "train_df = train_df.drop_duplicates(subset = 'News')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "# Train labels\n",
    "X_train= train_df['News'].astype(str)\n",
    "y_train = train_df['Sentiment']\n",
    "\n",
    "# Test labels\n",
    "X_test= new_df['News'].astype(str)\n",
    "y_test = new_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b087e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count Vectorizer Method \n",
    "cv = CountVectorizer()\n",
    "ctmTr = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Method\n",
    "model = RandomForestClassifier(n_estimators=100, criterion ='entropy', random_state = 0)\n",
    "model.fit(ctmTr, y_train)\n",
    "y_pred = model.predict(X_test_dtm)\n",
    "\n",
    "#Creating the confusion matrix for Random Forest\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "model.score(X_test_dtm,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a3984",
   "metadata": {},
   "source": [
    "## Code which need not be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52261145",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n",
    "\n",
    "# Basic function to get tokens from the text \n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    return mytokens\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parser = spacy.load(\"en_core_web_sm\")\n",
    "#spacy_tokenizer = parser.tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2)) \n",
    "classifier = LinearSVC()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b27ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Using Tfidf\n",
    "tfvectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer,ngram_range=(1,2))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vectorizer= TfidfVectorizer()\n",
    "tf_x_train = vectorizer.fit_transform(X_train)\n",
    "tf_x_test = vectorizer.transform(X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "clf = LogisticRegression(max_iter=1000,solver='saga')\n",
    "clf.fit(tf_x_train,y_train)\n",
    "y_test_pred=clf.predict(tf_x_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test_pred, y_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Create the  pipeline to clean, tokenize, vectorize, and classify using\"Count Vectorizor\"\n",
    "pipe_countvect = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', tfvectorizer),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Fit our data\n",
    "pipe_countvect.fit(X_train,y_train)\n",
    "\n",
    "# Predicting with a test dataset\n",
    "sample_prediction = pipe_countvect.predict(X_test)\n",
    "\n",
    "# Prediction Results\n",
    "# 0 = Positive review\n",
    "# 1 = Negative review\n",
    "for (sample,pred) in zip(X_test,sample_prediction):\n",
    "    print(sample,\"Prediction=>\",pred)\n",
    "    \n",
    "# Accuracy\n",
    "print(\"Accuracy: \",pipe_countvect.score(X_test,y_test))\n",
    "print(\"Accuracy: \",pipe_countvect.score(X_test,sample_prediction))\n",
    "# Accuracy\n",
    "print(\"Accuracy: \",pipe_countvect.score(X_train,y_train))\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
